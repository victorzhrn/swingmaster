# Tennis App - Core Architecture & Implementation Guide

## System Overview

The app processes tennis swings from video (live or recorded) using Apple's Vision framework for pose detection, analyzes the motion patterns, and provides coaching feedback.

## Core Architecture

```
Input Layer → Processing Pipeline → Storage Layer
     ↓              ↓                    ↓
Camera/File → Pose Detection → Analysis Results
           → Frame Buffer → Swing Detection
```

## File Structure

```
TennisApp/
├── Core/
│   ├── PoseProcessor.swift       // Vision API wrapper
│   ├── SwingDetector.swift       // Swing detection logic
│   ├── MetricsCalculator.swift   // Physics calculations
│   └── VideoProcessor.swift      // Unified video processing
├── Models/
│   ├── PoseFrame.swift          // Raw pose data
│   ├── SwingSegment.swift       // Detected swing data
│   └── AnalysisResult.swift     // Final analysis
├── Storage/
│   ├── CoreDataManager.swift    // Database operations
│   ├── VideoStorage.swift       // File management
│   └── TennisApp.xcdatamodeld   // Core Data schema
├── Views/
│   ├── CameraView.swift         // Live recording
│   ├── AnalysisView.swift       // Results display
│   └── HistoryView.swift        // Past sessions
└── Utils/
    ├── FrameBuffer.swift        // Circular buffer
    └── SignalProcessor.swift   // Smoothing filters
```

## Core Data Structures

### 1. PoseFrame.swift
```swift
struct PoseFrame {
    let timestamp: TimeInterval
    let joints: [VNHumanBodyPoseObservation.JointName: CGPoint]
    let confidence: [VNHumanBodyPoseObservation.JointName: Float]
    
    // Key joints for tennis
    static let trackedJoints: [VNHumanBodyPoseObservation.JointName] = [
        .rightWrist, .rightElbow, .rightShoulder,
        .leftWrist, .leftElbow, .leftShoulder,
        .rightHip, .leftHip, .root, .neck
    ]
    
    func getJoint(_ name: VNHumanBodyPoseObservation.JointName) -> CGPoint? {
        joints[name]
    }
}
```

### 2. SwingSegment.swift
```swift
struct SwingSegment {
    let id: UUID
    let startTime: TimeInterval
    let endTime: TimeInterval
    let type: SwingType
    let frames: [PoseFrame]
    let videoURL: URL?
    
    enum SwingType: String {
        case forehand, backhand, serve, unknown
    }
    
    var duration: TimeInterval {
        endTime - startTime
    }
    
    var peakVelocityFrame: Int? {
        // Frame with highest wrist velocity
    }
}
```

### 3. AnalysisResult.swift
```swift
struct AnalysisResult: Codable {
    let shotId: UUID
    let timestamp: Date
    let swingType: SwingSegment.SwingType
    let metrics: SwingMetrics
    let insights: [Insight]
    let score: Float // 0-10
    
    struct SwingMetrics: Codable {
        let maxWristSpeed: Float // m/s
        let contactPoint: CGPoint
        let swingPath: [CGPoint] // Simplified path
        let backswingAngle: Float
        let followThroughHeight: Float
    }
    
    struct Insight: Codable {
        let type: InsightType
        let message: String
        let severity: Severity
        
        enum InsightType: String, Codable {
            case contactPoint, followThrough, timing, posture
        }
        
        enum Severity: String, Codable {
            case good, warning, issue
        }
    }
}
```

## Key Components

### 1. PoseProcessor.swift
```swift
class PoseProcessor {
    private let visionQueue = DispatchQueue(label: "com.app.vision")
    private var poseRequest: VNDetectHumanBodyPoseRequest!
    
    init() {
        poseRequest = VNDetectHumanBodyPoseRequest()
        poseRequest.revision = VNDetectHumanBodyPoseRequestRevision1
    }
    
    func processFrame(_ pixelBuffer: CVPixelBuffer) async -> PoseFrame? {
        // Extract pose from single frame
        // Convert Vision coordinates to normalized coordinates
        // Return PoseFrame with joints and confidence
    }
    
    func processVideoFile(_ url: URL) async -> [PoseFrame] {
        // Extract frames at 15 fps
        // Process each frame
        // Return array of PoseFrames
    }
}
```

### 2. SwingDetector.swift
```swift
protocol SwingDetectorDelegate: AnyObject {
    func swingStartDetected(at timestamp: TimeInterval)
    func swingEndDetected(segment: SwingSegment)
}

class SwingDetector {
    private let frameBuffer = CircularBuffer<PoseFrame>(capacity: 90) // 3 seconds at 30fps
    private var detectionState = DetectionState.idle
    weak var delegate: SwingDetectorDelegate?
    
    enum DetectionState {
        case idle
        case possibleSwing(startIndex: Int)
        case inSwing(startIndex: Int, peakIndex: Int?)
    }
    
    func addFrame(_ frame: PoseFrame) {
        frameBuffer.append(frame)
        updateDetection()
    }
    
    private func updateDetection() {
        switch detectionState {
        case .idle:
            if detectSwingStart() {
                detectionState = .possibleSwing(startIndex: frameBuffer.count - 1)
            }
        case .possibleSwing(let startIndex):
            if detectSwingPeak() {
                detectionState = .inSwing(startIndex: startIndex, peakIndex: frameBuffer.count - 1)
            } else if frameBuffer.count - startIndex > 45 { // 1.5 second timeout
                detectionState = .idle
            }
        case .inSwing(let startIndex, _):
            if detectSwingEnd() {
                let segment = extractSegment(from: startIndex, to: frameBuffer.count - 1)
                delegate?.swingEndDetected(segment: segment)
                detectionState = .idle
            }
        }
    }
    
    private func detectSwingStart() -> Bool {
        // Check wrist velocity > threshold
        // Check shoulder rotation started
        // Return true if swing initiated
    }
    
    private func detectSwingPeak() -> Bool {
        // Find maximum wrist velocity
        // Verify it's above minimum threshold
    }
    
    private func detectSwingEnd() -> Bool {
        // Check velocity dropped below threshold
        // Check follow-through completed
    }
}
```

### 3. MetricsCalculator.swift
```swift
class MetricsCalculator {
    // Smoothing window size
    private let velocityWindowSize = 5
    
    func calculateWristSpeed(frames: [PoseFrame], joint: VNHumanBodyPoseObservation.JointName) -> [Float] {
        // Calculate velocity between consecutive frames
        // Apply moving average smoothing
        // Return array of speeds in m/s
    }
    
    func findContactPoint(frames: [PoseFrame]) -> (frame: Int, position: CGPoint)? {
        // Find frame with peak wrist velocity
        // Get wrist position at that frame
        // Return frame index and position
    }
    
    func calculateSwingPath(frames: [PoseFrame]) -> [CGPoint] {
        // Extract wrist positions
        // Simplify path using Douglas-Peucker algorithm
        // Return simplified path (10-15 points)
    }
    
    func calculateBodyAngles(frame: PoseFrame) -> BodyAngles {
        // Calculate shoulder rotation
        // Calculate hip alignment
        // Calculate spine angle
    }
    
    struct BodyAngles {
        let shoulderRotation: Float
        let hipRotation: Float
        let spineAngle: Float
    }
}
```

### 4. VideoProcessor.swift
```swift
class VideoProcessor {
    private let poseProcessor = PoseProcessor()
    private let swingDetector = SwingDetector()
    private let metricsCalculator = MetricsCalculator()
    
    enum Source {
        case live(AVCaptureSession)
        case file(URL)
    }
    
    func process(source: Source) async -> [AnalysisResult] {
        switch source {
        case .live(let session):
            return await processLiveSession(session)
        case .file(let url):
            return await processVideoFile(url)
        }
    }
    
    private func processVideoFile(_ url: URL) async -> [AnalysisResult] {
        // Extract frames at 15fps
        let frames = await poseProcessor.processVideoFile(url)
        
        // Detect swings
        var swings: [SwingSegment] = []
        for frame in frames {
            swingDetector.addFrame(frame)
        }
        
        // Analyze each swing
        return swings.map { analyzeSwing($0) }
    }
    
    private func analyzeSwing(_ segment: SwingSegment) -> AnalysisResult {
        // Calculate metrics
        let metrics = metricsCalculator.calculateMetrics(segment.frames)
        
        // Generate insights
        let insights = generateInsights(metrics)
        
        // Calculate score
        let score = calculateScore(metrics, insights)
        
        return AnalysisResult(
            shotId: segment.id,
            timestamp: Date(),
            swingType: segment.type,
            metrics: metrics,
            insights: insights,
            score: score
        )
    }
}
```

### 5. Storage Components

#### CoreDataManager.swift
```swift
class CoreDataManager {
    lazy var persistentContainer: NSPersistentContainer = {
        let container = NSPersistentContainer(name: "TennisApp")
        container.loadPersistentStores { _, error in
            if let error = error {
                fatalError("Core Data failed: \(error)")
            }
        }
        return container
    }()
    
    func saveSession(_ results: [AnalysisResult], videoURL: URL) {
        // Create Session entity
        // Create Shot entities for each result
        // Save to Core Data
    }
    
    func fetchRecentSessions(limit: Int = 20) -> [Session] {
        // Fetch recent sessions
        // Include shots relationship
    }
}
```

#### Core Data Schema (TennisApp.xcdatamodeld)
```
Session
├── id: UUID
├── date: Date
├── videoPath: String
└── shots: [Shot] (relationship)

Shot
├── id: UUID
├── sessionId: UUID
├── startTime: Double
├── endTime: Double
├── type: String
├── score: Float
├── metricsJSON: String
└── session: Session (relationship)
```

#### VideoStorage.swift
```swift
class VideoStorage {
    private let documentsDirectory = FileManager.default.urls(
        for: .documentDirectory,
        in: .userDomainMask
    ).first!
    
    func saveVideo(from tempURL: URL) -> URL {
        let fileName = "\(UUID().uuidString).mp4"
        let destination = documentsDirectory.appendingPathComponent(fileName)
        try? FileManager.default.moveItem(at: tempURL, to: destination)
        return destination
    }
    
    func deleteVideo(at url: URL) {
        try? FileManager.default.removeItem(at: url)
    }
    
    func getVideoSize() -> Int64 {
        // Calculate total size of all videos
    }
}
```

## Data Flow

### Live Recording Flow
```
1. AVCaptureVideoDataOutput → CMSampleBuffer
2. Convert to CVPixelBuffer
3. PoseProcessor.processFrame() → PoseFrame
4. SwingDetector.addFrame()
5. On swing detection → Extract segment
6. MetricsCalculator → Generate metrics
7. Save to CoreData + VideoStorage
```

### Video Upload Flow
```
1. User selects video → URL
2. VideoProcessor.process(source: .file(url))
3. Extract frames at 15fps
4. Batch process through PoseProcessor
5. Feed frames to SwingDetector
6. Analyze detected swings
7. Save results to CoreData
```

## Performance Considerations

1. **Frame Processing**: Process every 3rd frame for live (10fps effective)
2. **Memory**: Keep maximum 90 frames in buffer (3 seconds)
3. **Background Processing**: Use Task for video analysis
4. **Batch Operations**: Process uploaded videos in chunks of 150 frames
5. **Storage Cleanup**: Implement video retention policy (e.g., 30 days)

## Implementation Order

1. PoseProcessor - Vision API integration
2. FrameBuffer - Circular buffer for 4-second window
3. PeakDetector - Find velocity peaks above threshold
4. SwingClassifier - 3-frame validation (can start with rules, add Gemini later)
5. BoundaryFinder - Backward/forward traversal from peak
6. MetricsCalculator - Speed and position metrics
7. VideoProcessor - Orchestrate all components
8. Storage layer - CoreData + file management
9. UI integration - Connect to views

## Testing Strategy

1. Unit test SwingDetector with mock pose data
2. Test MetricsCalculator with known inputs
3. Integration test with sample tennis videos
4. Performance test with 2-minute videos
5. Memory test with continuous recording