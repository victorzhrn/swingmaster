# Tennis App - YOLO Object Detection Implementation Plan

## Overview
Add real-time tennis ball and racket detection using YOLO model alongside existing Vision pose detection. The implementation follows your established architecture patterns with clean separation of concerns.

## Current State Assessment
âœ… **Already Implemented:**
- Vision API pose detection (`PoseProcessor.swift`)
- Camera capture with frame processing (`CameraManager.swift`)
- Skeleton overlay visualization (`SkeletonOverlay.swift`)
- Full video processing pipeline (`VideoProcessor.swift`)
- Recording with pose overlay (`CameraView.swift`)

ðŸŽ¯ **To Add:**
- YOLO tennis ball and racket detection
- Visual overlays for detected objects
- Integration with existing analysis pipeline

---

## Phase 1: YOLO Model Integration (3-4 hours)

### 1.1 Create TennisObjectDetector
**Location:** `Core/TennisObjectDetector.swift`

```swift
// Parallel to PoseProcessor, handles YOLO detection
class TennisObjectDetector {
    private var detectionRequest: VNCoreMLRequest?
    private let detectionQueue = DispatchQueue(label: "com.swingmaster.yolo", qos: .userInitiated)
    
    struct Detection {
        let racketBox: CGRect?      // Normalized coordinates
        let ballBox: CGRect?        // Normalized coordinates
        let racketConfidence: Float
        let ballConfidence: Float
        let timestamp: TimeInterval
    }
    
    init() {
        setupModel()
    }
    
    private func setupModel() {
        // Load your downloaded .mlmodel
        guard let modelURL = Bundle.main.url(forResource: "yolov8n", withExtension: "mlmodelc") else {
            print("Failed to find YOLO model")
            return
        }
        
        guard let model = try? VNCoreMLModel(for: MLModel(contentsOf: modelURL)) else {
            print("Failed to load YOLO model")
            return
        }
        
        detectionRequest = VNCoreMLRequest(model: model)
        detectionRequest?.imageCropAndScaleOption = .scaleFit
    }
    
    func detectObjects(_ pixelBuffer: CVPixelBuffer, timestamp: TimeInterval) async -> Detection? {
        // Process frame and return detections
    }
}
```

**Tasks:**
- [ ] Create TennisObjectDetector class
- [ ] Load YOLO .mlmodel from bundle
- [ ] Set up VNCoreMLRequest
- [ ] Implement async detection method
- [ ] Parse YOLO results for "tennis racket" and "sports ball" classes
- [ ] Return normalized bounding boxes

### 1.2 Create Detection Data Models
**Location:** `Models/ObjectDetection.swift`

```swift
struct RacketDetection: Sendable {
    let boundingBox: CGRect    // Normalized 0-1
    let confidence: Float
    let timestamp: TimeInterval
}

struct BallDetection: Sendable {
    let boundingBox: CGRect    // Normalized 0-1
    let confidence: Float
    let timestamp: TimeInterval
}

struct ObjectDetectionFrame: Sendable {
    let timestamp: TimeInterval
    let racket: RacketDetection?
    let ball: BallDetection?
}
```

---

## Phase 2: Update CameraManager (2-3 hours)

### 2.1 Integrate Object Detection
**Location:** `Managers/CameraManager.swift`

**Updates needed:**
```swift
class CameraManager: NSObject, ObservableObject {
    // Existing properties...
    
    // ADD: Object detection
    private let objectDetector = TennisObjectDetector()
    @Published var latestRacket: RacketDetection?
    @Published var latestBall: BallDetection?
    
    // In captureOutput delegate method:
    func captureOutput(_ output: AVCaptureOutput, 
                      didOutput sampleBuffer: CMSampleBuffer, 
                      from connection: AVCaptureConnection) {
        frameCount += 1
        if frameCount % 3 != 0 { return }  // Process at 10fps
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        let timestamp = CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer))
        
        // Parallel processing
        Task { [weak self] in
            guard let self = self else { return }
            
            // Run both detections concurrently
            async let poseTask = self.poseProcessor.processFrame(pixelBuffer, timestamp: timestamp)
            async let objectTask = self.objectDetector.detectObjects(pixelBuffer, timestamp: timestamp)
            
            let (pose, objects) = await (poseTask, objectTask)
            
            await MainActor.run {
                self.latestPose = pose
                self.latestRacket = objects?.racket
                self.latestBall = objects?.ball
            }
        }
    }
}
```

**Tasks:**
- [ ] Add TennisObjectDetector instance
- [ ] Add @Published properties for racket and ball detections
- [ ] Update captureOutput to run both detections in parallel
- [ ] Publish detection results to UI

---

## Phase 3: Create Object Overlay Components (2-3 hours)

### 3.1 Create RacketOverlay
**Location:** `Components/RacketOverlay.swift`

```swift
struct RacketOverlay: View {
    let detection: RacketDetection?
    
    var body: some View {
        GeometryReader { geo in
            if let detection = detection {
                Rectangle()
                    .stroke(TennisColors.tennisYellow, lineWidth: 3)
                    .frame(
                        width: detection.boundingBox.width * geo.size.width,
                        height: detection.boundingBox.height * geo.size.height
                    )
                    .position(
                        x: detection.boundingBox.midX * geo.size.width,
                        y: (1 - detection.boundingBox.midY) * geo.size.height
                    )
                    .overlay(
                        Text("Racket \(Int(detection.confidence * 100))%")
                            .font(.caption2)
                            .foregroundColor(.tennisYellow)
                            .padding(2)
                            .background(Color.black.opacity(0.6))
                            .cornerRadius(4)
                            .position(
                                x: detection.boundingBox.minX * geo.size.width + 40,
                                y: (1 - detection.boundingBox.maxY) * geo.size.height - 10
                            )
                    )
                    .animation(.smooth(duration: 0.1), value: detection.boundingBox)
            }
        }
        .allowsHitTesting(false)
    }
}
```

### 3.2 Create BallOverlay
**Location:** `Components/BallOverlay.swift`

```swift
struct BallOverlay: View {
    let detection: BallDetection?
    
    var body: some View {
        GeometryReader { geo in
            if let detection = detection {
                Circle()
                    .stroke(TennisColors.tennisYellow, lineWidth: 2)
                    .background(
                        Circle()
                            .fill(TennisColors.tennisYellow.opacity(0.2))
                    )
                    .frame(
                        width: detection.boundingBox.width * geo.size.width,
                        height: detection.boundingBox.height * geo.size.height
                    )
                    .position(
                        x: detection.boundingBox.midX * geo.size.width,
                        y: (1 - detection.boundingBox.midY) * geo.size.height
                    )
                    .animation(.smooth(duration: 0.1), value: detection.boundingBox)
            }
        }
        .allowsHitTesting(false)
    }
}
```

**Tasks:**
- [ ] Create RacketOverlay component
- [ ] Create BallOverlay component
- [ ] Handle coordinate conversion (Vision uses bottom-left origin)
- [ ] Add confidence labels
- [ ] Smooth animations between frames

---

## Phase 4: Update CameraView (1-2 hours)

### 4.1 Add Object Overlays
**Location:** `Views/CameraView.swift`

```swift
struct CameraView: View {
    @StateObject private var camera = CameraManager()
    @State private var showSkeleton: Bool = true
    @State private var showObjects: Bool = true  // NEW
    
    var body: some View {
        ZStack {
            // Camera preview...
            
            // Skeleton overlay (existing)
            if showSkeleton {
                SkeletonOverlay(pose: camera.latestPose)
                    .ignoresSafeArea()
                    .transition(.opacity)
            }
            
            // ADD: Object overlays
            if showObjects {
                RacketOverlay(detection: camera.latestRacket)
                    .ignoresSafeArea()
                    .transition(.opacity)
                
                BallOverlay(detection: camera.latestBall)
                    .ignoresSafeArea()
                    .transition(.opacity)
            }
            
            // Controls...
        }
        .overlay(alignment: .topTrailing) {
            // ADD: Toggle buttons
            HStack(spacing: 8) {
                Button(action: { withAnimation { showSkeleton.toggle() } }) {
                    Image(systemName: showSkeleton ? "figure.walk" : "figure.walk.slash")
                        .foregroundColor(showSkeleton ? .white : .gray)
                }
                .buttonStyle(OverlayToggleStyle())
                
                Button(action: { withAnimation { showObjects.toggle() } }) {
                    Image(systemName: "tennisball")
                        .foregroundColor(showObjects ? .tennisYellow : .gray)
                }
                .buttonStyle(OverlayToggleStyle())
            }
            .padding()
        }
    }
}
```

**Tasks:**
- [ ] Add showObjects state variable
- [ ] Add RacketOverlay and BallOverlay to view hierarchy
- [ ] Create toggle button for object detection
- [ ] Ensure overlays don't interfere with controls

---

## Phase 5: Contact Point Detection (2-3 hours)

### 5.1 Create ContactPointDetector
**Location:** `Core/ContactPointDetector.swift`

```swift
class ContactPointDetector {
    struct ContactEvent {
        let timestamp: TimeInterval
        let position: CGPoint       // Normalized
        let confidence: Float
        let racketBox: CGRect
        let ballBox: CGRect
    }
    
    private var lastBallPosition: CGPoint?
    private var lastRacketBox: CGRect?
    
    func detectContact(racket: RacketDetection?, 
                       ball: BallDetection?) -> ContactEvent? {
        guard let racket = racket, 
              let ball = ball,
              racket.confidence > 0.7,
              ball.confidence > 0.5 else { return nil }
        
        // Check if ball is within or near racket bounds
        let expandedRacket = racket.boundingBox.insetBy(dx: -0.02, dy: -0.02)
        let ballCenter = CGPoint(
            x: ball.boundingBox.midX,
            y: ball.boundingBox.midY
        )
        
        if expandedRacket.contains(ballCenter) {
            return ContactEvent(
                timestamp: racket.timestamp,
                position: ballCenter,
                confidence: min(racket.confidence, ball.confidence),
                racketBox: racket.boundingBox,
                ballBox: ball.boundingBox
            )
        }
        
        return nil
    }
}
```

**Tasks:**
- [ ] Create ContactPointDetector class
- [ ] Implement proximity detection algorithm
- [ ] Track ball trajectory for better contact detection
- [ ] Add to CameraManager for real-time detection

---

## Phase 6: Integrate with VideoProcessor (3-4 hours)

### 6.1 Update VideoProcessor
**Location:** `Core/VideoProcessor.swift`

```swift
public final class VideoProcessor: ObservableObject {
    private let poseProcessor = PoseProcessor()
    private let objectDetector = TennisObjectDetector()  // NEW
    private let contactDetector = ContactPointDetector() // NEW
    
    public func processVideo(_ url: URL) async -> [AnalysisResult] {
        // 1. Extract poses AND objects
        state = .extractingPoses(progress: 0)
        
        let (poseFrames, objectFrames) = await processVideoWithObjects(url)
        
        // 2. Calculate metrics with object data
        state = .calculatingMetrics
        let metrics = metricsCalculator.calculateMetrics(
            for: poseFrames,
            objects: objectFrames  // NEW: Include object data
        )
        
        // 3. Detect swings with enhanced data
        state = .detectingSwings
        let potentialSwings = swingDetector.detectPotentialSwings(
            frames: poseFrames,
            metrics: metrics,
            objects: objectFrames  // NEW: Use for validation
        )
        
        // Continue with validation and analysis...
    }
    
    private func processVideoWithObjects(_ url: URL) async -> ([PoseFrame], [ObjectDetectionFrame]) {
        // Process video frames for both pose and objects
        // Return synchronized arrays
    }
}
```

**Tasks:**
- [ ] Add TennisObjectDetector to VideoProcessor
- [ ] Update processVideo to extract objects alongside poses
- [ ] Pass object data to MetricsCalculator
- [ ] Use object detection to improve swing validation

---

## Phase 7: Enhanced Metrics with Object Data (2-3 hours)

### 7.1 Update MetricsCalculator
**Location:** `Core/MetricsCalculator.swift`

```swift
extension MetricsCalculator {
    func calculateMetrics(for frames: [PoseFrame], 
                         objects: [ObjectDetectionFrame]) -> EnhancedFrameMetrics {
        // Existing metrics...
        let baseMetrics = calculateMetrics(for: frames)
        
        // NEW: Object-based metrics
        let racketSpeed = calculateRacketSpeed(objects)
        let racketPath = extractRacketPath(objects)
        let contactPoints = detectAllContacts(objects)
        
        return EnhancedFrameMetrics(
            base: baseMetrics,
            racketSpeed: racketSpeed,
            racketPath: racketPath,
            contactPoints: contactPoints
        )
    }
    
    private func calculateRacketSpeed(_ objects: [ObjectDetectionFrame]) -> [Float] {
        // Calculate speed from racket position changes
    }
}
```

**Tasks:**
- [ ] Extend MetricsCalculator to process object data
- [ ] Calculate racket speed from bounding box movement
- [ ] Extract racket swing path
- [ ] Identify contact points from ball-racket proximity

---

## Testing & Validation Plan

### Unit Tests
- [ ] Test YOLO model loading and inference
- [ ] Test coordinate transformations
- [ ] Test contact point detection logic
- [ ] Test parallel processing performance

### Integration Tests
- [ ] Test real-time detection on device
- [ ] Verify overlay synchronization
- [ ] Test with various lighting conditions
- [ ] Validate detection accuracy

### Performance Tests
- [ ] Measure frame processing time
- [ ] Monitor memory usage with dual detection
- [ ] Test battery impact
- [ ] Verify 10+ fps maintained

---

## Implementation Order

**Week 1: Core Detection**
1. Day 1-2: Implement TennisObjectDetector
2. Day 3: Integrate with CameraManager
3. Day 4: Create overlay components
4. Day 5: Update CameraView with overlays

**Week 2: Analysis Integration**
1. Day 1-2: Add contact point detection
2. Day 3-4: Update VideoProcessor
3. Day 5: Enhance MetricsCalculator

**Week 3: Polish & Optimization**
1. Day 1-2: Performance optimization
2. Day 3: Edge case handling
3. Day 4-5: Testing and refinement

---

## Key Files to Create

```
swingmaster/
â”œâ”€â”€ Core/
â”‚   â”œâ”€â”€ TennisObjectDetector.swift     # NEW
â”‚   â””â”€â”€ ContactPointDetector.swift     # NEW
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ ObjectDetection.swift          # NEW
â”œâ”€â”€ Components/
â”‚   â”œâ”€â”€ RacketOverlay.swift           # NEW
â”‚   â””â”€â”€ BallOverlay.swift              # NEW
```

## Files to Modify

```
â”œâ”€â”€ Managers/
â”‚   â””â”€â”€ CameraManager.swift           # Add object detection
â”œâ”€â”€ Views/
â”‚   â””â”€â”€ CameraView.swift              # Add overlays
â”œâ”€â”€ Core/
â”‚   â”œâ”€â”€ VideoProcessor.swift          # Process objects
â”‚   â””â”€â”€ MetricsCalculator.swift       # Enhanced metrics
```

---

## Success Criteria

âœ… **Real-time Detection**
- Racket detected in 80%+ of frames where visible
- Ball detected when in frame
- Overlays update smoothly at 10+ fps

âœ… **Contact Detection**
- Identifies ball-racket contact within 3 frames
- Low false positive rate (<10%)

âœ… **Performance**
- Processing time <100ms per frame
- Memory usage <50MB additional
- Battery impact minimal

âœ… **User Experience**
- Toggle overlays independently
- Clear visual feedback
- No interference with recording

---

## Notes & Considerations

### Model Selection
- Using YOLOv8n (located at project root as `yolov8n.mlmodel`)
- Xcode generates a class named `yolov8n` automatically
- Can upgrade to YOLOv8s for better accuracy if needed (replace the .mlmodel file)
- Consider fine-tuning on tennis-specific dataset later

### Coordinate Systems
- Vision/YOLO: Origin at bottom-left, y increases upward
- SwiftUI: Origin at top-left, y increases downward
- Always convert: `swiftUI_y = 1 - vision_y`

### Edge Cases
- Multiple balls in frame (practice sessions)
- Partial racket visibility
- Motion blur during fast swings
- Poor lighting conditions

### Future Enhancements
- Track ball trajectory for spin analysis
- Detect racket face angle (open/closed)
- Identify grip type from hand position
- Multi-player detection and tracking