# Tennis App Vision API Implementation Plan

## Overview
Progressive implementation plan to add real pose detection using iOS Vision framework and Gemini Flash 2.0 for intelligent swing validation. Architecture emphasizes clean separation of concerns with each module having a single responsibility.

---

## 🏗️ Architecture Overview

```
Camera/Video → PoseProcessor → MetricsCalculator → SwingDetector → GeminiValidator → AnalysisResult
                    ↓                ↓                   ↓              ↓
              (Vision API)    (All calculations)  (Peak detection)  (AI validation)
```

### Core Principles
- **MetricsCalculator**: Owns ALL metric calculations (single source of truth)
- **SwingDetector**: Only detects peaks and extracts segments
- **GeminiValidator**: Handles AI validation with 30-frame context
- **Clear boundaries**: Each module is independently testable and replaceable
  - Smoothing is implemented inside `MetricsCalculator` (no separate SignalProcessor in MVP)

---

## 🚀 Phase 1: Vision Framework Foundation
**Duration:** 4-6 hours  
**Goal:** Set up core Vision API integration and verify pose detection works

### 1.1 Create PoseProcessor.swift
Location: `Core/PoseProcessor.swift`

**Implementation Tasks:**
- [ ] Create PoseProcessor class with Vision framework imports
- [ ] Set up VNDetectHumanBodyPoseRequest
- [ ] Implement frame processing for single images
- [ ] Add coordinate conversion (Vision → UIKit)
- [ ] Create PoseFrame data structure

**Key Code Structure:**
```swift
class PoseProcessor {
    private let visionQueue = DispatchQueue(label: "com.swingmaster.vision", qos: .userInitiated)
    private var bodyPoseRequest: VNDetectHumanBodyPoseRequest!
    
    // Process single frame
    func processFrame(_ pixelBuffer: CVPixelBuffer) async -> PoseFrame?
    
    // Process video file
    func processVideoFile(_ url: URL) async -> [PoseFrame]
}

struct PoseFrame {
    let timestamp: TimeInterval
    let joints: [VNHumanBodyPoseObservation.JointName: CGPoint]
    let confidence: [VNHumanBodyPoseObservation.JointName: Float]
}
```

### 1.2 Create Core Data Models
Location: `Models/`

**Files to Create:**
- [ ] `PoseFrame.swift` - Raw pose data from Vision
- [ ] `SwingSegment.swift` - Persisted, validated swing with final start/end frames
- [ ] `AnalysisResult.swift` - Final analysis output

### 1.3 Test Harness
- [ ] Create simple test view with static image
- [ ] Verify pose detection returns 19 joints
- [ ] Log confidence scores and positions
- [ ] **TEST ON REAL DEVICE** (Won't work in simulator)

**Success Criteria:**
✅ Can detect pose from static image  
✅ Returns normalized coordinates for 19 joints  
✅ Confidence scores > 0.5 for visible joints

---

## 🎥 Phase 2: Live Camera Integration
**Duration:** 6-8 hours  
**Goal:** Show real-time skeleton overlay on camera feed

### 2.1 Enhance CameraManager
Location: `Managers/CameraManager.swift`

**Implementation Tasks:**
- [ ] Add AVCaptureVideoDataOutput alongside existing movie output
- [ ] Implement AVCaptureVideoDataOutputSampleBufferDelegate
- [ ] Set up dual output (recording + frame processing)
- [ ] Configure pixel format and queue
- [ ] Process every 3rd frame (10fps from 30fps source)

**Key Changes:**
```swift
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, 
                      didOutput sampleBuffer: CMSampleBuffer, 
                      from connection: AVCaptureConnection) {
        // Process for skeleton overlay
        frameCount += 1
        if frameCount % 3 == 0 { // Process at 10fps
            // Send to PoseProcessor
        }
    }
}
```

### 2.2 Create SkeletonOverlay Component
Location: `Components/SkeletonOverlay.swift`

**Features to Implement:**
- [ ] Convert Vision coordinates to view coordinates
- [ ] Draw lines between connected joints
- [ ] Color code by confidence (green > 0.8, yellow > 0.5, red < 0.5)
- [ ] Smooth animation between frames
- [ ] Handle device orientation

**Joint Connections:**
```
Head: nose → neck
Arms: shoulder → elbow → wrist
Torso: neck → root
Legs: hip → knee → ankle
```

### 2.3 Update CameraView
- [ ] Add SkeletonOverlay to view hierarchy
- [ ] Toggle skeleton visibility
- [ ] Performance monitoring (fps counter)
- [ ] Maintain recording functionality

**Success Criteria:**
✅ Skeleton tracks user in real-time  
✅ Maintains 10+ fps on iPhone 12 or newer  
✅ No lag in recording functionality  
✅ Skeleton disappears when not detecting person

---

## 📊 Phase 3: Centralized Metrics System
**Duration:** 4-5 hours  
**Goal:** Build comprehensive metrics calculator as single source of truth

### 3.1 MetricsCalculator Implementation
Location: `Core/MetricsCalculator.swift`

**Core Responsibilities:**
- Calculate ALL metrics from pose frames
- Provide clean interface for consumers
- Easy to extend with new metrics

```swift
class MetricsCalculator {
    // Window sizes for smoothing
    private let velocityWindowSize = 5
    private let angleWindowSize = 3
    
    func calculateMetrics(for frames: [PoseFrame]) -> FrameMetrics {
        return FrameMetrics(
            angularVelocities: calculateAngularVelocity(frames),
            linearVelocities: calculateLinearVelocity(frames),
            jointAngles: calculateJointAngles(frames),
            shoulderRotation: calculateShoulderRotation(frames),
            hipRotation: calculateHipRotation(frames),
            wristHeights: calculateWristHeights(frames)
        )
    }
    
    // Individual metric calculations
    func calculateAngularVelocity(_ frames: [PoseFrame]) -> [Float] {
        // Calculate wrist angular velocity around elbow-shoulder axis
        // Apply smart moving average for smoothing
        // Return rad/s for each frame
    }
    
    func calculateLinearVelocity(_ frames: [PoseFrame]) -> [Float] {
        // Calculate wrist linear speed
        // Smooth with moving window
        // Return m/s for each frame
    }
    
    // Segment-specific metrics for analysis
    func calculateSegmentMetrics(for segment: [PoseFrame]) -> SegmentMetrics {
        return SegmentMetrics(
            peakAngularVelocity: // max angular velocity
            peakLinearVelocity: // max linear velocity
            contactPoint: // position at peak velocity
            backswingAngle: // max shoulder rotation during backswing
            followThroughHeight: // max wrist height after contact
            averageConfidence: // pose detection confidence
        )
    }
}

struct FrameMetrics {
    let angularVelocities: [Float]
    let linearVelocities: [Float]
    let jointAngles: [JointAngles]
    let shoulderRotation: [Float]
    let hipRotation: [Float]
    let wristHeights: [Float]
    // Easy to add more metrics here
}

struct SegmentMetrics {
    let peakAngularVelocity: Float
    let peakLinearVelocity: Float
    let contactPoint: CGPoint
    let backswingAngle: Float
    let followThroughHeight: Float
    let averageConfidence: Float
}
```

### 3.2 Testing Metrics
- [ ] Test with known motion patterns
- [ ] Verify angular velocity spikes on swings
- [ ] Validate smoothing doesn't lose peaks
- [ ] Compare with manual measurements

**Success Criteria:**
✅ Angular velocity peaks > 3.0 rad/s for swings  
✅ Smooth curves without jitter  
✅ Metrics computed in < 50ms for 90 frames  
✅ Easy to add new metrics

---

## 🎾 Phase 4: Simplified Swing Detection
**Duration:** 3-4 hours  
**Goal:** Detect potential swings using peak detection only

### 4.1 SwingDetector Implementation
Location: `Core/SwingDetector.swift`

**Single Responsibility: Peak detection and candidate extraction**

```swift
class SwingDetector {
    private let peakThreshold: Float = 3.0 // rad/s threshold
    private let minPeakSeparation = 1.0 // seconds between swings
    
    // Segment extraction parameters
    private let segmentDuration = 1.0 // total seconds
    private let beforePeak = 0.7 // seconds before peak
    private let afterPeak = 0.3 // seconds after peak
    
    func detectPotentialSwings(frames: [PoseFrame], 
                               metrics: FrameMetrics) -> [PotentialSwing] {
        // 1. Find peaks in angular velocity
        let peaks = findPeaks(
            in: metrics.angularVelocities,
            threshold: peakThreshold,
            minSeparation: minPeakSeparation
        )
        
        // 2. Extract 30-frame segments around each peak
        return peaks.compactMap { peakIndex in
            extractSegment(
                peakIndex: peakIndex,
                frames: frames,
                metrics: metrics
            )
        }
    }
    
    private func findPeaks(in velocities: [Float], 
                          threshold: Float,
                          minSeparation: TimeInterval) -> [Int] {
        // Find local maxima above threshold
        // Ensure minimum time separation
        // Return frame indices
    }
    
    private func extractSegment(peakIndex: Int, 
                               frames: [PoseFrame],
                               metrics: FrameMetrics) -> PotentialSwing? {
        // Calculate frame indices
        let framesAt30fps = 30 // frames per second
        let framesBefore = Int(beforePeak * Double(framesAt30fps)) // 21 frames
        let framesAfter = Int(afterPeak * Double(framesAt30fps)) // 9 frames
        
        let startIndex = max(0, peakIndex - framesBefore)
        let endIndex = min(frames.count - 1, peakIndex + framesAfter)
        
        // Ensure we have enough frames
        guard endIndex - startIndex >= 20 else { return nil } // Need at least 20 frames
        
        return PotentialSwing(
            frames: Array(frames[startIndex...endIndex]),
            peakFrameIndex: peakIndex - startIndex,
            peakVelocity: metrics.angularVelocities[peakIndex],
            timestamp: frames[peakIndex].timestamp
        )
    }
}

struct PotentialSwing {
    let frames: [PoseFrame] // ~30 frames for Gemini
    let peakFrameIndex: Int // Peak position within segment
    let peakVelocity: Float // Angular velocity at peak
    let timestamp: TimeInterval // Original video timestamp
}
```

### 4.2 Testing Peak Detection
- [ ] Test with recorded swings
- [ ] Verify no false positives from walking
- [ ] Validate segment extraction
- [ ] Check edge cases (start/end of video)

**Success Criteria:**
✅ Detects 90%+ of obvious swings  
✅ < 20% false positives  
✅ Segments contain full swing motion  
✅ Handles video boundaries correctly

---

## 🤖 Phase 5: Gemini AI Validation
**Duration:** 5-6 hours  
**Goal:** Use Gemini Flash 2.0 for accurate swing validation and analysis

### 5.1 GeminiValidator Implementation
Location: `Core/GeminiValidator.swift`

**Responsibilities:**
- Validate potential swings with 30-frame context
- Get precise start/end timestamps from AI
- Generate coaching insights
 - Map validated frames into the app's canonical `SwingSegment` for persistence

```swift
class GeminiValidator {
    private let apiKey = "YOUR_GEMINI_API_KEY"
    private let model = "gemini-2.0-flash-exp"
    
    // MARK: - Swing Validation (30 frames)
    
    func validateSwing(_ potential: PotentialSwing) async throws -> ValidatedSwing? {
        // 1. Convert all 30 frames to base64 images
        let images = try await prepareImages(from: potential.frames)
        
        // 2. Create validation prompt
        let prompt = """
        Analyze these 30 frames (1 second of video at 30fps) showing a potential tennis swing.
        The peak motion occurs at frame \(potential.peakFrameIndex).
        
        Tasks:
        1. Confirm if this is a valid tennis swing
        2. Identify the EXACT frame number where the backswing starts
        3. Identify the EXACT frame number where the follow-through ends
        4. Classify the swing type (forehand/backhand/serve)
        
        Return JSON:
        {
            "is_valid_swing": boolean,
            "swing_type": "forehand|backhand|serve|unknown",
            "start_frame": number (0-29),
            "end_frame": number (0-29),
            "confidence": number (0.0-1.0)
        }
        """
        
        // 3. Send to Gemini
        let response = try await callGeminiAPI(images: images, prompt: prompt)
        
        // 4. Parse and validate response
        guard let validation = parseValidationResponse(response),
              validation.isValid else { return nil }
        
        // 5. Extract exact frames based on Gemini's timestamps
        let startIdx = validation.startFrame
        let endIdx = validation.endFrame
        
        return ValidatedSwing(
            frames: Array(potential.frames[startIdx...endIdx]),
            type: validation.swingType,
            confidence: validation.confidence,
            originalTimestamp: potential.timestamp
        )
    }
    
    // MARK: - Swing Analysis (5 key frames + metrics)
    
    func analyzeSwing(_ swing: ValidatedSwing, 
                      metrics: SegmentMetrics) async throws -> AnalysisResult {
        // 1. Select 5 key frames
        let keyFrames = selectKeyFrames(from: swing.frames)
        let images = try await prepareImages(from: keyFrames)
        
        // 2. Create analysis prompt with metrics
        let prompt = """
        Analyze this validated tennis \(swing.type) swing.
        
        Metrics from motion analysis:
        - Peak angular velocity: \(String(format: "%.2f", metrics.peakAngularVelocity)) rad/s
        - Peak linear velocity: \(String(format: "%.2f", metrics.peakLinearVelocity)) m/s
        - Contact point: X=\(metrics.contactPoint.x), Y=\(metrics.contactPoint.y)
        - Shoulder rotation: \(String(format: "%.1f", metrics.backswingAngle))°
        - Follow-through height: \(String(format: "%.2f", metrics.followThroughHeight))
        
        Provide coaching analysis:
        1. Rate the contact point (early/late/perfect)
        2. Evaluate follow-through quality
        3. Assess body rotation and weight transfer
        4. Give overall form score (0-10)
        5. Provide 2-3 specific improvement tips
        
        Return JSON:
        {
            "contact_rating": "early|late|perfect",
            "contact_adjustment": number (inches to adjust),
            "follow_through_rating": "low|good|excellent",
            "rotation_rating": "insufficient|good|excellent",
            "form_score": number (0-10),
            "insights": [
                {
                    "type": "contact|follow_through|rotation|balance",
                    "message": string,
                    "priority": "high|medium|low"
                }
            ]
        }
        """
        
        // 3. Get analysis from Gemini
        let response = try await callGeminiAPI(images: images, prompt: prompt)
        
        // 4. Parse and return results
        return parseAnalysisResponse(response, swing: swing, metrics: metrics)
    }
    
    // MARK: - Helper Methods
    
    private func prepareImages(from frames: [PoseFrame]) async throws -> [String] {
        // Convert frames to 480p images
        // Encode as base64
        // Return array of base64 strings
    }
    
    private func selectKeyFrames(from frames: [PoseFrame]) -> [PoseFrame] {
        // Select 5 representative frames:
        // 1. Start of backswing
        // 2. Mid-backswing
        // 3. Contact point
        // 4. Mid-follow-through
        // 5. End position
    }
}

struct ValidatedSwing {
    let frames: [PoseFrame]
    let type: SwingType
    let confidence: Float
    let originalTimestamp: TimeInterval
}

// Mapping to persisted model (performed by VideoProcessor or a mapper):
// ValidatedSwing -> SwingSegment (final start/end, type, timestamps)
```

### 5.2 API Integration
- [ ] Set up Gemini API client
- [ ] Handle authentication
- [ ] Implement retry logic
- [ ] Add rate limiting
- [ ] Cache responses for similar swings

### 5.3 Error Handling
- [ ] Network failure recovery
- [ ] Invalid API responses
- [ ] Timeout handling (10s max)
- [ ] Fallback to local detection

**Success Criteria:**
✅ Validation accuracy > 85%  
✅ Response time < 2 seconds  
✅ Graceful degradation on failure  
✅ Clear, actionable insights

---

## 🎬 Phase 6: Video Processing Pipeline
**Duration:** 4-5 hours  
**Goal:** Orchestrate all components for complete video analysis

### 6.1 VideoProcessor Implementation
Location: `Core/VideoProcessor.swift`

```swift
class VideoProcessor {
    private let poseProcessor = PoseProcessor()
    private let metricsCalculator = MetricsCalculator()
    private let swingDetector = SwingDetector()
    private let geminiValidator = GeminiValidator()
    
    enum ProcessingState {
        case extractingPoses(progress: Float)
        case calculatingMetrics
        case detectingSwings
        case validatingSwings(current: Int, total: Int)
        case analyzingSwings(current: Int, total: Int)
        case complete
    }
    
    @Published var state: ProcessingState = .extractingPoses(progress: 0)
    
    func processVideo(_ url: URL) async throws -> [AnalysisResult] {
        // 1. Extract poses from all frames
        state = .extractingPoses(progress: 0)
        let poseFrames = try await poseProcessor.processVideoFile(url) { progress in
            state = .extractingPoses(progress: progress)
        }
        
        // 2. Calculate metrics centrally
        state = .calculatingMetrics
        let metrics = metricsCalculator.calculateMetrics(for: poseFrames)
        
        // 3. Detect potential swings
        state = .detectingSwings
        let potentialSwings = swingDetector.detectPotentialSwings(
            frames: poseFrames,
            metrics: metrics
        )
        
        // 4. Validate each swing with Gemini
        var validatedSwings: [ValidatedSwing] = []
        for (index, potential) in potentialSwings.enumerated() {
            state = .validatingSwings(current: index + 1, total: potentialSwings.count)
            
            if let validated = try await geminiValidator.validateSwing(potential) {
                validatedSwings.append(validated)
            }
        }
        
        // 5. Analyze validated swings
        var results: [AnalysisResult] = []
        for (index, swing) in validatedSwings.enumerated() {
            state = .analyzingSwings(current: index + 1, total: validatedSwings.count)
            
            let segmentMetrics = metricsCalculator.calculateSegmentMetrics(
                for: swing.frames
            )
            let analysis = try await geminiValidator.analyzeSwing(
                swing,
                metrics: segmentMetrics
            )
            results.append(analysis)
        }
        
        // 6. Map validated swings to SwingSegment and persist results
        // (Storage handled by CoreDataManager / VideoStorage per Architecture Guide)
        // Save AnalysisResult with associated media URL

        state = .complete
        return results
    }
    
    // For live camera recording
    func processLiveSession(_ frames: [PoseFrame]) async throws -> [AnalysisResult] {
        // Similar pipeline but frames are already collected
        // Can process in parallel since we have all frames
    }
}
```

### 6.2 Queue Management
- [ ] Queue API calls to avoid rate limits
- [ ] Batch processing for multiple swings
- [ ] Priority queue for live recordings
- [ ] Background processing support
 - [ ] Use a small `FrameBuffer` during live capture to maintain ~3s of context

### 6.3 Progress Reporting
- [ ] Granular progress updates
- [ ] Time estimates
- [ ] Cancel support
- [ ] Pause/resume capability

**Success Criteria:**
✅ Process 1-minute video in < 15 seconds  
✅ Clear progress indication  
✅ Handles interruptions gracefully  
✅ Memory usage < 200MB

---

## 🔧 Phase 7: Integration & Polish
**Duration:** 4-5 hours  
**Goal:** Connect everything to existing UI and handle edge cases

### 7.1 Connect to Existing UI
- [ ] Update MockShot → real SwingSegment
- [ ] Wire VideoProcessor to AnalysisView
- [ ] Update timeline with actual timestamps
- [ ] Display real metrics in insight cards

### 7.2 Edge Case Handling
- [ ] Multiple people in frame (choose closest to center)
- [ ] Poor lighting (warn user)
- [ ] Partial body visibility (skip frames)
- [ ] Non-tennis movements (filtered by Gemini)
- [ ] Very long videos (segment processing)

### 7.3 Performance Optimization
- [ ] Profile with Instruments
- [ ] Optimize image encoding for Gemini
- [ ] Implement frame caching
- [ ] Parallel processing where possible

### 7.4 User Experience
- [ ] Loading states with helpful messages
- [ ] Error messages with recovery actions
- [ ] Smooth transitions between states
- [ ] Haptic feedback on detection

**Success Criteria:**
✅ Seamless integration with existing UI  
✅ Graceful handling of all edge cases  
✅ Consistent performance across devices  
✅ Professional, polished feel

---

## 📅 Implementation Timeline

### Week 1: Foundation
**Mon-Tue:** Phase 1 - PoseProcessor and models  
**Wed-Thu:** Phase 2 - Live camera and skeleton overlay  
**Fri:** Testing on real device, performance baseline

### Week 2: Intelligence
**Mon:** Phase 3 - MetricsCalculator (all calculations)  
**Tue:** Phase 4 - SwingDetector (peak detection only)  
**Wed-Thu:** Phase 5 - GeminiValidator integration  
**Fri:** End-to-end testing with real swings

### Week 3: Production
**Mon-Tue:** Phase 6 - VideoProcessor orchestration  
**Wed-Thu:** Phase 7 - UI integration and polish  
**Fri:** User testing and iterations

---

## 🎯 Key Architecture Benefits

### 1. **Single Responsibility**
Each module has ONE clear job:
- **PoseProcessor**: Vision API interface
- **MetricsCalculator**: ALL calculations
- **SwingDetector**: Peak detection only
- **GeminiValidator**: AI validation
- **VideoProcessor**: Orchestration

### 2. **Easy to Extend**
Add new metrics? Just update MetricsCalculator:
```swift
// Future additions - just add to MetricsCalculator
func calculateRacquetAngle(_ frames: [PoseFrame]) -> [Float]
func calculateWeightTransfer(_ frames: [PoseFrame]) -> [Float]
func calculateStanceWidth(_ frames: [PoseFrame]) -> [Float]
```

### 3. **Testable**
Each component can be tested in isolation:
```swift
// Test examples
testMetricsCalculator.testAngularVelocityCalculation()
testSwingDetector.testPeakDetection()
mockGeminiValidator.testWithMockResponses()
```

### 4. **Replaceable**
Easy to swap implementations:
- Replace Gemini with on-device ML later
- Try different peak detection algorithms
- Add alternative validators

---

## 🚨 Critical Notes

### Device Requirements
- **MUST test on real device** - Vision body pose doesn't work in simulator
- **Minimum iPhone 12** for good performance
- **iOS 14.0+** for VNDetectHumanBodyPoseRequest

### API Considerations
- **Gemini API key required** - Get from Google AI Studio
- **Rate limits**: ~60 requests per minute
- **Costs**: ~$0.001-0.003 per swing analysis
- **Network required**: No offline mode in MVP

### Privacy
- **User consent** for sending video frames to Gemini
- **Downsample images** to 480p before sending
- **No PII** in API requests
- **Option to disable** cloud processing

---

## 📊 Success Metrics

### MVP Targets
- **Detection Rate**: 80%+ of actual swings detected
- **False Positives**: < 20%
- **Validation Accuracy**: 85%+ with Gemini
- **Processing Speed**: < 15 seconds for 1-minute video
- **API Cost**: < $0.02 per recording session
- **User Satisfaction**: 4.0+ star rating

### Future Optimization
- On-device ML model trained on Gemini responses
- Offline mode for basic detection
- Real-time coaching during recording
- Multi-angle support

