# YOLO11 iOS Integration - Phase 2 Implementation Plan

## Overview
This document provides a step-by-step implementation plan for integrating YOLO11 into your Tennis App, replacing the existing YOLO3 implementation.

## Prerequisites Completed (Phase 1)
- [x] YOLO11 model exported to CoreML format (`yolo11l.mlpackage`)
- [x] Model includes NMS (Non-Maximum Suppression)
- [x] Model tested in Python environment

---

## Implementation Steps

### Step 1: Add YOLO11 Model to Xcode Project (Day 1)

#### 1.1 Remove Old YOLO3 Model
```bash
# From your Xcode project navigator:
1. Select "YOLOv3 FP16.mlmodelc" or "YOLOv3 FP16.mlmodel"
2. Right-click → "Delete" → "Move to Trash"
3. Clean build folder: Product → Clean Build Folder (⇧⌘K)
```

#### 1.2 Add YOLO11 Model
```bash
1. Drag "yolo11l.mlpackage" into Xcode project
2. Ensure "Copy items if needed" is checked
3. Add to targets: "swingmaster"
4. Verify in Build Phases → Copy Bundle Resources
```

#### 1.3 Verify Model Integration
```swift
// Quick check in your app entry
if let url = Bundle.main.url(forResource: "yolo11l", withExtension: "mlmodelc") {
    print("✅ YOLO11 model found at: \(url)")
} else {
    print("❌ YOLO11 model not found in bundle")
}
```

**Checkpoint:** Build and run to ensure model is accessible

---

### Step 2: Update TennisObjectDetector.swift (Day 1-2)

#### 2.1 Backup Current Implementation
```bash
# Create backup before modifications
cp Core/TennisObjectDetector.swift Core/TennisObjectDetector_YOLO3_backup.swift
```

#### 2.2 Update Model Loading
Replace the existing `setupModel()` method (and simplify state handling):

```swift
import Foundation
import Vision
import CoreML
import CoreVideo
import ImageIO

class TennisObjectDetector {
    private var detectionRequest: VNCoreMLRequest?
    private var visionModel: VNCoreMLModel?
    private let detectionQueue = DispatchQueue(label: "com.swingmaster.yolo11", qos: .userInitiated)

    // Store latest detections for simple handoff
    private var latestRacketDetection: RacketDetection?
    private var latestBallDetection: BallDetection?

    struct Detection {
        let racketBox: CGRect?
        let ballBox: CGRect?
        let racketConfidence: Float
        let ballConfidence: Float
        let timestamp: TimeInterval
    }

    init() {
        setupModel()
    }

    private func setupModel() {
        // Check if running in preview
        let isPreview = ProcessInfo.processInfo.environment["XCODE_RUNNING_FOR_PREVIEWS"] == "1"

        if isPreview {
            print("Running in Preview - skipping YOLO11 model load")
            return
        }

        // Load compiled Core ML model from bundle
        guard let modelURL = Bundle.main.url(forResource: "yolo11l", withExtension: "mlmodelc") else {
            print("❌ Failed to find YOLO11 model in bundle")
            return
        }

        do {
            var config = MLModelConfiguration()
            config.computeUnits = .all // Prefer ANE/GPU when available
            let mlModel = try MLModel(contentsOf: modelURL, configuration: config)
            visionModel = try VNCoreMLModel(for: mlModel)

            // Create request with completion handler
            detectionRequest = VNCoreMLRequest(model: visionModel!) { [weak self] request, error in
                self?.processDetections(for: request, error: error)
            }

            // Preserve aspect without distortion; Vision will resize as needed
            detectionRequest?.imageCropAndScaleOption = .centerCrop

            print("✅ YOLO11 model loaded successfully")
        } catch {
            print("❌ Failed to load YOLO11 model: \(error)")
        }
    }
}
```

#### 2.3 Implement Detection Processing
Add the new detection processing method:

```swift
private func processDetections(for request: VNRequest, error: Error?) {
    guard error == nil else {
        print("Detection error: \(error!)")
        return
    }
    
    guard let observations = request.results as? [VNRecognizedObjectObservation] else {
        return
    }
    
    // YOLO11 label names (prefer names over numeric IDs)
    // Ball: "sports ball", "tennis ball" (if custom)
    // Racket: "tennis racket"
    
    var racketBox: CGRect?
    var ballBox: CGRect?
    var racketConfidence: Float = 0
    var ballConfidence: Float = 0
    
    for observation in observations {
        // Get the top label for this detection
        guard let topLabel = observation.labels.first else { continue }
        
        let className = topLabel.identifier
        let confidence = topLabel.confidence
        
        // Check for tennis racket (name-based)
        if className == "tennis racket" {
            if confidence > racketConfidence {
                racketBox = observation.boundingBox
                racketConfidence = confidence
            }
        }
        // Check for ball (name-based)
        else if className == "sports ball" || className == "tennis ball" {
            if confidence > ballConfidence {
                ballBox = observation.boundingBox
                ballConfidence = confidence
            }
        }
    }
    
    // Update published properties
    DispatchQueue.main.async {
        if let box = racketBox {
            self.latestRacketDetection = RacketDetection(
                boundingBox: box,
                confidence: racketConfidence,
                timestamp: CACurrentMediaTime()
            )
        } else {
            self.latestRacketDetection = nil
        }
        
        if let box = ballBox {
            self.latestBallDetection = BallDetection(
                boundingBox: box,
                confidence: ballConfidence,
                timestamp: CACurrentMediaTime()
            )
        } else {
            self.latestBallDetection = nil
        }
    }
}
```

#### 2.4 Update Detection Method
Replace the existing `detectObjects` method:

```swift
func detectObjects(_ pixelBuffer: CVPixelBuffer, 
                  timestamp: TimeInterval, 
                  orientation: CGImagePropertyOrientation = .up) async -> Detection? {
    
    guard let request = detectionRequest else {
        print("Detection request not initialized")
        return nil
    }
    
    return await withCheckedContinuation { continuation in
        let handler = VNImageRequestHandler(
            cvPixelBuffer: pixelBuffer,
            orientation: orientation,
            options: [:]
        )
        
        detectionQueue.async {
            do {
                // Perform the detection
                try handler.perform([request])
                
                // Create detection result from latest detections
                let detection = Detection(
                    racketBox: self.latestRacketDetection?.boundingBox,
                    ballBox: self.latestBallDetection?.boundingBox,
                    racketConfidence: self.latestRacketDetection?.confidence ?? 0,
                    ballConfidence: self.latestBallDetection?.confidence ?? 0,
                    timestamp: timestamp
                )
                
                continuation.resume(returning: detection)
            } catch {
                print("Failed to perform detection: \(error)")
                continuation.resume(returning: nil)
            }
        }
    }
}
```

**Testing Checkpoint:**
```swift
// Add test method to verify detection
func testDetection(with image: UIImage) {
    guard let cgImage = image.cgImage else { return }
    
    let requestHandler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    
    do {
        try requestHandler.perform([detectionRequest!])
        print("Test detection complete")
    } catch {
        print("Test detection failed: \(error)")
    }
}
```

---

### Step 3: Update CameraManager.swift (Day 2)

#### 3.1 Modify Frame Processing
Update the `captureOutput` method to handle YOLO11 results and robust orientation:

```swift
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, 
                      didOutput sampleBuffer: CMSampleBuffer, 
                      from connection: AVCaptureConnection) {
        frameCount += 1
        // Process every 3rd frame for ~10fps from 30fps source
        if frameCount % 3 != 0 { return }
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        let timestamp = CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer))
        
        // Determine orientation based on device orientation and camera
        let isFrontCamera = false // Set accordingly if you support front camera
        let orientation: CGImagePropertyOrientation = visionOrientation(
            from: UIDevice.current.orientation,
            isFront: isFrontCamera
        )
        
        Task { [weak self] in
            guard let self = self else { return }
            
            // Run detections concurrently
            async let poseTask = self.poseProcessor.processFrame(pixelBuffer, timestamp: timestamp)
            async let objectTask = self.objectDetector.detectObjects(
                pixelBuffer, 
                timestamp: timestamp, 
                orientation: orientation
            )
            
            let (pose, objectDetection) = await (poseTask, objectTask)
            
            await MainActor.run {
                // Update pose
                self.latestPose = pose
                
                // Update object detections with confidence threshold
                if let detection = objectDetection {
                    // Only update if confidence is above threshold
                    let minConfidence: Float = 0.3
                    
                    if let box = detection.racketBox, detection.racketConfidence > minConfidence {
                        self.latestRacket = RacketDetection(
                            boundingBox: box,
                            confidence: detection.racketConfidence,
                            timestamp: detection.timestamp
                        )
                    } else {
                        self.latestRacket = nil
                    }
                    
                    if let box = detection.ballBox, detection.ballConfidence > minConfidence {
                        self.latestBall = BallDetection(
                            boundingBox: box,
                            confidence: detection.ballConfidence,
                            timestamp: detection.timestamp
                        )
                    } else {
                        self.latestBall = nil
                    }
                }
            }
        }
        
        // Update FPS counter
        updateFPS()
    }
}
```

Add this helper to map device orientation to Vision orientation (adjust `isFront` based on the active camera):

```swift
func visionOrientation(from device: UIDeviceOrientation, isFront: Bool) -> CGImagePropertyOrientation {
    switch device {
    case .portrait:           return isFront ? .leftMirrored  : .right
    case .portraitUpsideDown: return isFront ? .rightMirrored : .left
    case .landscapeLeft:      return isFront ? .downMirrored  : .up
    case .landscapeRight:     return isFront ? .upMirrored    : .down
    default:                  return isFront ? .leftMirrored  : .right
    }
}
```

---

### Step 4: Update UI Overlays (Day 2-3)

#### 4.1 Update RacketOverlay.swift
Ensure coordinate transformation is correct for YOLO11:

```swift
struct RacketOverlay: View {
    let detection: RacketDetection?
    
    var body: some View {
        GeometryReader { geo in
            if let detection = detection {
                let rect = convertVisionToSwiftUI(
                    detection.boundingBox, 
                    in: geo.size
                )
                
                Rectangle()
                    .stroke(TennisColors.tennisYellow, lineWidth: 3)
                    .frame(width: rect.width, height: rect.height)
                    .position(x: rect.midX, y: rect.midY)
                    .overlay(
                        confidenceLabel(detection.confidence, at: rect)
                    )
                    .animation(.smooth(duration: 0.1), value: detection.boundingBox)
            }
        }
        .allowsHitTesting(false)
    }
    
    private func convertVisionToSwiftUI(_ visionRect: CGRect, in size: CGSize) -> CGRect {
        // YOLO11 with Vision returns normalized coordinates (0-1)
        // Origin at bottom-left, need to flip Y for SwiftUI
        let x = visionRect.origin.x * size.width
        let y = (1 - visionRect.origin.y - visionRect.height) * size.height
        let width = visionRect.width * size.width
        let height = visionRect.height * size.height
        
        return CGRect(x: x, y: y, width: width, height: height)
    }
    
    private func confidenceLabel(_ confidence: Float, at rect: CGRect) -> some View {
        Text("Racket \(Int(confidence * 100))%")
            .font(.caption2)
            .foregroundColor(.tennisYellow)
            .padding(2)
            .background(Color.black.opacity(0.6))
            .cornerRadius(4)
            .position(x: rect.minX + 40, y: rect.minY - 10)
    }
}
```

#### 4.2 Update BallOverlay.swift
Similar updates for ball overlay:

```swift
struct BallOverlay: View {
    let detection: BallDetection?
    
    var body: some View {
        GeometryReader { geo in
            if let detection = detection {
                let rect = convertVisionToSwiftUI(
                    detection.boundingBox, 
                    in: geo.size
                )
                
                Circle()
                    .stroke(TennisColors.tennisYellow, lineWidth: 2)
                    .background(
                        Circle()
                            .fill(TennisColors.tennisYellow.opacity(0.2))
                    )
                    .frame(width: rect.width, height: rect.height)
                    .position(x: rect.midX, y: rect.midY)
                    .animation(.smooth(duration: 0.1), value: detection.boundingBox)
            }
        }
        .allowsHitTesting(false)
    }
    
    private func convertVisionToSwiftUI(_ visionRect: CGRect, in size: CGSize) -> CGRect {
        // Same conversion as RacketOverlay
        let x = visionRect.origin.x * size.width
        let y = (1 - visionRect.origin.y - visionRect.height) * size.height
        let width = visionRect.width * size.width
        let height = visionRect.height * size.height
        
        return CGRect(x: x, y: y, width: width, height: height)
    }
}
```

---

### Step 5: Update VideoProcessor.swift (Day 3)

#### 5.1 Modify Video Frame Processing
Update the `processVideoWithObjects` method (let Vision scale; no manual 640×640):

```swift
private func processVideoWithObjects(_ url: URL) async -> ([PoseFrame], [ObjectDetectionFrame]) {
    let asset = AVAsset(url: url)
    
    guard let videoTrack = asset.tracks(withMediaType: .video).first else {
        print("No video track found")
        return ([], [])
    }
    
    // Create asset reader
    guard let reader = try? AVAssetReader(asset: asset) else {
        print("Failed to create asset reader")
        return ([], [])
    }
    
    // Configure output; Vision will handle resizing via imageCropAndScaleOption
    let outputSettings: [String: Any] = [
        kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
    ]
    
    let videoOutput = AVAssetReaderTrackOutput(
        track: videoTrack,
        outputSettings: outputSettings
    )
    
    reader.add(videoOutput)
    reader.startReading()
    
    var poseFrames: [PoseFrame] = []
    var objectFrames: [ObjectDetectionFrame] = []
    var frameIndex = 0
    
    while reader.status == .reading {
        guard let sampleBuffer = videoOutput.copyNextSampleBuffer(),
              let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            continue
        }
        
        let timestamp = CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer))
        
        // Process every 3rd frame to match real-time processing
        if frameIndex % 3 == 0 {
            // Run detections
            async let poseTask = poseProcessor.processFrame(pixelBuffer, timestamp: timestamp)
            async let objectTask = objectDetector.detectObjects(
                pixelBuffer, 
                timestamp: timestamp,
                orientation: .up
            )
            
            let (poseFrame, objectDetection) = await (poseTask, objectTask)
            
            if let pose = poseFrame {
                poseFrames.append(pose)
            }
            
            if let detection = objectDetection {
                let objectFrame = ObjectDetectionFrame(
                    timestamp: timestamp,
                    racket: detection.racketBox != nil ? RacketDetection(
                        boundingBox: detection.racketBox!,
                        confidence: detection.racketConfidence,
                        timestamp: timestamp
                    ) : nil,
                    ball: detection.ballBox != nil ? BallDetection(
                        boundingBox: detection.ballBox!,
                        confidence: detection.ballConfidence,
                        timestamp: timestamp
                    ) : nil
                )
                objectFrames.append(objectFrame)
            }
        }
        
        frameIndex += 1
        
        // Update progress
        if frameIndex % 30 == 0 {
            let progress = Float(frameIndex) / Float(videoTrack.nominalFrameRate * Float(asset.duration.seconds))
            await MainActor.run {
                self.state = .extractingPoses(progress: min(progress, 1.0))
            }
        }
    }
    
    return (poseFrames, objectFrames)
}
```

---

### Step 6: Update ContactPointDetector.swift (Day 3)

#### 6.1 Enhance Contact Detection for YOLO11
Update contact detection with better confidence handling:

```swift
class ContactPointDetector {
    struct ContactEvent {
        let timestamp: TimeInterval
        let position: CGPoint       // Normalized
        let confidence: Float
        let racketBox: CGRect
        let ballBox: CGRect
    }
    
    private var lastBallPosition: CGPoint?
    private var lastRacketBox: CGRect?
    private let iouThreshold: Float = 0.3
    
    func detectContact(racket: RacketDetection?, 
                      ball: BallDetection?) -> ContactEvent? {
        // Require minimum confidence for YOLO11 detections
        guard let racket = racket, 
              let ball = ball,
              racket.confidence > 0.5,  // Higher threshold for YOLO11
              ball.confidence > 0.4 else { 
            return nil 
        }
        
        // Calculate IoU (Intersection over Union)
        let intersection = racket.boundingBox.intersection(ball.boundingBox)
        let union = racket.boundingBox.union(ball.boundingBox)
        let iou = intersection.area / union.area
        
        // Check for contact based on IoU or proximity
        if iou > iouThreshold {
            return ContactEvent(
                timestamp: racket.timestamp,
                position: CGPoint(
                    x: ball.boundingBox.midX,
                    y: ball.boundingBox.midY
                ),
                confidence: min(racket.confidence, ball.confidence),
                racketBox: racket.boundingBox,
                ballBox: ball.boundingBox
            )
        }
        
        // Alternative: Check if ball center is within expanded racket bounds
        let expandedRacket = racket.boundingBox.insetBy(dx: -0.03, dy: -0.03)
        let ballCenter = CGPoint(
            x: ball.boundingBox.midX,
            y: ball.boundingBox.midY
        )
        
        if expandedRacket.contains(ballCenter) {
            return ContactEvent(
                timestamp: racket.timestamp,
                position: ballCenter,
                confidence: min(racket.confidence, ball.confidence) * 0.9, // Slightly lower confidence
                racketBox: racket.boundingBox,
                ballBox: ball.boundingBox
            )
        }
        
        // Update tracking
        lastBallPosition = ballCenter
        lastRacketBox = racket.boundingBox
        
        return nil
    }
}

// Extension for CGRect area calculation
extension CGRect {
    var area: CGFloat {
        return width * height
    }
}
```

---

## Testing Plan

### Day 4: Integration Testing

#### Test 1: Model Loading
```swift
func testModelLoading() {
    let detector = TennisObjectDetector()
    // Should print "✅ YOLO11 model loaded successfully"
}
```

#### Test 2: Real-time Camera Detection
```swift
func testCameraDetection() {
    // Open CameraView
    // Point at tennis racket and ball
    // Verify overlays appear with >30% confidence
    // Check FPS remains above 10
}
```

#### Test 3: Video Processing
```swift
func testVideoProcessing() {
    // Upload a tennis video
    // Verify detection during processing
    // Check that rackets and balls are identified
}
```

#### Test 4: Performance Metrics
```swift
func measurePerformance() {
    // Record inference time per frame
    // Target: <100ms per frame on iPhone 12+
    // Memory usage: <200MB additional
    // Battery drain: <10% for 5-minute session
}
```

---

## Rollback Strategy

If issues arise, you can quickly rollback:

1. **Restore YOLO3 Model:**
```bash
git checkout Core/TennisObjectDetector_YOLO3_backup.swift
mv Core/TennisObjectDetector_YOLO3_backup.swift Core/TennisObjectDetector.swift
```

2. **Re-add YOLO3 Model to Xcode:**
   - Add back "YOLOv3 FP16.mlmodelc"
   - Remove "yolo11n.mlpackage"

3. **Revert Changes:**
```bash
git checkout HEAD -- CameraManager.swift VideoProcessor.swift
```

---

## Troubleshooting Guide

### Issue: No Detections
**Solution:**
- Check model loaded successfully (console logs)
- Verify confidence thresholds (try lowering to 0.2)
- Test with well-lit, clear tennis scenes
- Ensure correct orientation parameter

### Issue: Wrong Objects Detected
**Solution:**
- Verify class IDs (32 for ball, 38 for racket)
- Check label.identifier values in debugger
- Consider custom training if needed

### Issue: Poor Performance
**Solution:**
- Prefer `yolo11n` (nano) for real-time camera; use `yolo11l` for offline/video if needed
- Check frame sampling rate (process every 3rd frame)
- Verify Neural Engine is enabled (not CPU-only)
- Profile with Instruments

### Issue: Coordinate Mismatch
**Solution:**
- Verify Vision to SwiftUI conversion (Y-axis flip)
- Check aspect ratio handling
- Test with known coordinates

### Issue: App Crashes
**Solution:**
- Check memory usage in Instruments
- Verify all optionals are safely unwrapped
- Add more error handling in detection pipeline
- Check for retain cycles

---

## Success Metrics

✅ **Phase 2 Complete When:**
- [ ] YOLO11 model loads without errors
- [ ] Tennis rackets detected with >50% accuracy
- [ ] Tennis balls detected with >40% accuracy  
- [ ] Real-time detection maintains >10 FPS
- [ ] Video processing completes successfully
- [ ] Contact points detected when ball meets racket
- [ ] All existing features work with new model
- [ ] Performance metrics meet or exceed YOLO3

---

## Next Steps (Phase 3)

After completing Phase 2:
1. Collect performance metrics
2. Gather test footage results
3. Consider custom training if accuracy needs improvement
4. Optimize for battery life if needed
5. Prepare for production deployment

---

## Resources & References

- [YOLO11 Documentation](https://docs.ultralytics.com/models/yolo11/)
- [Vision Framework Guide](https://developer.apple.com/documentation/vision)
- [CoreML Best Practices](https://developer.apple.com/documentation/coreml)
- [Your Project Repository](swingmaster/)

---

*Last Updated: Implementation Plan for YOLO3 → YOLO11 Migration*