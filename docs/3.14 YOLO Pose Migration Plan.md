## YOLO v11 Pose Migration Plan (Replace Vision Pose End-to-End)

### Overview
Completely replace Apple Vision body pose with the YOLO v11 Pose CoreML model. Eliminate the Vision-based `PoseProcessor` and ensure all downstream consumers (metrics, swing detection, overlays, pipeline) work with YOLO pose outputs. The plan preserves the overall pipeline and UI while decoupling our data model from Vision.

### Goals
- Use only YOLO v11 pose for all pose extraction (live camera and file).
- Remove the Vision pose implementation and dependencies from runtime code paths.
- Keep outputs compatible with existing UI and analysis with minimal refactors.

### Non-Goals
- No hybrid/fallback between Vision and YOLO.
- No server-side processing; remains fully on-device.

---

## Architecture Changes

1) Data Model Decoupling (from Vision)
- Introduce `BodyJoint` (app-defined) to replace `VNHumanBodyPoseObservation.JointName` everywhere.
- Update `PoseFrame` to use `BodyJoint` for `joints` and `confidences` dictionaries.
- Keep normalized coordinates in Vision-style image space: x,y in [0,1], origin at bottom-left.

2) New Pose Backend
- Add `YOLOPoseProcessor` under `Core/` that owns model loading, inference, decoding, orientation handling, normalization, and `PoseFrame` construction.
- Delete the old Vision `PoseProcessor.swift` and remove references.

3) Pipeline Integration
- Update `VideoProcessor` to instantiate and use `YOLOPoseProcessor` for `processVideoFile`.
- Update live camera path in `CameraManager` to use `YOLOPoseProcessor` at ~10 fps for overlays.
- Update CLI tools in `Tools/` that reference pose extraction to call `YOLOPoseProcessor`.

4) UI and Consumers
- Update `SkeletonOverlay` to import no Vision symbols and use `BodyJoint`.
- Ensure `MetricsCalculator`, `SwingDetector`, and `TrajectoryComputer` read `BodyJoint`-keyed `PoseFrame` without further changes, except for renamed joint keys.

---

## Model Integration

1) Model Placement
- Move the pose model bundle into the app target, mirroring the object model placement:
  - From: `yolo/yolo11l-pose.mlpackage`
  - To: `swingmaster/yolo11l-pose.mlpackage`
- Add to the Xcode project and ensure the app target is checked under “Add to targets”.

2) Loading Strategy (mirrors `TennisObjectDetector`)
- Primary: `Bundle.main.url(forResource: "yolo11l-pose", withExtension: "mlmodelc")`.
- Optional override for tools: `YOLO_POSE_MODEL_PATH` environment variable (absolute path to compiled `.mlmodelc`).
- Compute units: `.cpuOnly` on Simulator; `.all` on device.
- `VNCoreMLRequest` with `imageCropAndScaleOption = .centerCrop`.

3) Output Decoding & Normalization
- Expected output: COCO-17 keypoints with per-keypoint confidence.
- Normalize to [0,1] in model image space; flip y to set origin at bottom-left.
- Multi-person: choose the highest-confidence instance (sum or mean of keypoint scores).

---

## Data Model Specification

1) `BodyJoint` (app-defined)
- Cases: `nose`, `leftEye`, `rightEye`, `leftEar`, `rightEar`, `neck`, `root`, `leftShoulder`, `rightShoulder`, `leftElbow`, `rightElbow`, `leftWrist`, `rightWrist`, `leftHip`, `rightHip`, `leftKnee`, `rightKnee`, `leftAnkle`, `rightAnkle`.

2) `PoseFrame`
- `timestamp: TimeInterval`
- `joints: [BodyJoint: CGPoint]` (normalized, origin bottom-left)
- `confidences: [BodyJoint: Float]`
- Encoding keeps String keys for portability (BodyJoint.rawValue)

3) COCO → BodyJoint Mapping
- 0 nose → nose
- 1 left_eye → leftEye
- 2 right_eye → rightEye
- 3 left_ear → leftEar
- 4 right_ear → rightEar
- 5 left_shoulder → leftShoulder
- 6 right_shoulder → rightShoulder
- 7 left_elbow → leftElbow
- 8 right_elbow → rightElbow
- 9 left_wrist → leftWrist
- 10 right_wrist → rightWrist
- 11 left_hip → leftHip
- 12 right_hip → rightHip
- 13 left_knee → leftKnee
- 14 right_knee → rightKnee
- 15 left_ankle → leftAnkle
- 16 right_ankle → rightAnkle
- Derived: `neck` = midpoint(leftShoulder, rightShoulder)
- Derived: `root` = midpoint(leftHip, rightHip)

---

## Implementation Steps

1) Move Model & Project Setup
- Move: `mv yolo/yolo11l-pose.mlpackage swingmaster/`
- Add to Xcode; verify it’s bundled in the target.
- Keep `.gitignore` ignoring `**/*.mlmodelc` (compiled outputs) — `*.mlpackage` remains committed.

2) Create `Core/YOLOPoseProcessor.swift`
- Load model (bundle first, env override optional) and create `VNCoreMLRequest`.
- Provide APIs:
  - `processFrame(_:timestamp:orientation:) async -> PoseFrame?`
  - `processVideoFile(_:targetFPS:orientation:progress:) async -> [PoseFrame]`
- Implement decode → normalize → COCO map → derive `neck`/`root` → build `PoseFrame`.

3) Replace Pipeline Usage
- `VideoProcessor`: replace any `PoseProcessor` usage with `YOLOPoseProcessor`.
- `CameraManager`: use `YOLOPoseProcessor` at ~10 fps for overlay updates.
- `Tools/ProcessProVideos*.swift`: update to the new processor.

4) Data Model Refactor
- Add `Models/BodyJoint.swift` (new enum).
- Update `Models/PoseFrame.swift` to use `BodyJoint` and String-keyed Codable.
- Update all references to `VNHumanBodyPoseObservation.JointName` → `BodyJoint`.

5) UI & Consumer Updates
- `Components/SkeletonOverlay.swift`: remove `import Vision`; change bone pairs to `[(BodyJoint, BodyJoint)]` and replace enum cases accordingly; keep drawing logic and thresholds.
- `Core/MetricsCalculator.swift`, `Core/SwingDetector.swift`, `Utils/TrajectoryComputer.swift`: rename joint keys to `BodyJoint` and keep algorithms unchanged.

6) Remove Old Vision Implementation
- Delete `Core/PoseProcessor.swift` and any dead code paths.
- Remove unused Vision imports from files that no longer need them.

7) Testing & QA
- Unit tests: mapping (COCO → BodyJoint), derived joints, coordinate normalization.
- Device tests: iPhone 12+ for ≥10 fps overlay; verify timestamps and progress updates.
- Golden video: compare wrist/shoulder trajectories pre/post migration for sanity.

8) Performance
- Maintain ~10 fps processing (every third 30 fps frame) in live mode.
- Use `.all` compute units on device; ensure no memory growth over long clips.

---

## Risks & Mitigations
- Keypoint semantics differ slightly between Vision and COCO: mitigate with robust mapping and derived joints.
- Orientation/normalization mistakes: add visual flip checks (e.g., wrist y increases upward) and unit tests.
- Multi-person scenes: pick the person whose hip/shoulder center is closest to frame center; configurable.

---

## Acceptance Criteria
- Vision pose code removed from runtime (no `PoseProcessor` usage; no Vision joint enums in models/UI).
- YOLO pose drives both file processing and live overlays.
- Skeleton overlay renders correctly with `BodyJoint` and looks consistent with prior visuals.
- Metrics and swing detection operate normally with YOLO pose input.

---

## Rollout Checklist
- [ ] Move `yolo11l-pose.mlpackage` into `swingmaster/` and add to target
- [ ] Add `BodyJoint.swift`; update `PoseFrame.swift` to use `BodyJoint`
- [ ] Create `YOLOPoseProcessor.swift` and verify model loads on device
- [ ] Replace `PoseProcessor` usages in `VideoProcessor`, `CameraManager`, `Tools/`
- [ ] Update `SkeletonOverlay`, `MetricsCalculator`, `SwingDetector`, `TrajectoryComputer`
- [ ] Delete old `PoseProcessor.swift` and remove unused Vision imports
- [ ] Device test: live overlay ≥10 fps; file processing works with progress
- [ ] Golden sample comparison and QA sign-off


