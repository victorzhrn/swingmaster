# Tennis App - Phased Implementation Plan

## Overview
Build a fully functional tennis coaching app with mock data in 5 progressive phases. Each phase delivers testable functionality that builds toward the complete experience.

---

## Phase 1: Static UI Shell & Navigation
**Duration:** 4-6 hours  
**Goal:** Create all screens with navigation flow

### Components to Build

#### 1. App Structure
```swift
TennisApp.swift
├── ContentView.swift (navigation controller)
├── CameraView.swift (placeholder camera)
├── HistoryView.swift (list of sessions)
└── AnalysisView.swift (video + insights)
```

#### 2. Navigation System
```swift
enum Screen {
    case camera
    case history
    case analysis(session: MockSession)
}

@State private var currentScreen: Screen = .camera
```

#### 3. Mock Data Models
```swift
struct MockSession {
    let id = UUID()
    let date: String
    let shotCount: Int
    let videoURL: String // placeholder
    let thumbnail: String // SF Symbol or color
}

struct Shot {
    let time: Double // seconds into video
    let type: ShotType // forehand/backhand
    let score: Float // 0-10
    let issue: String // "Late contact"
}
```

#### 4. UI Components
- **CameraView:** Black rectangle with camera icon, three buttons at bottom
- **HistoryView:** List with 5 hardcoded sessions
- **AnalysisView:** Video placeholder + static timeline + insight card

### Mock Data Provider
```swift
class MockDataProvider {
    static let sessions = [
        MockSession(date: "Today 2:30 PM", shotCount: 5, ...),
        MockSession(date: "Yesterday", shotCount: 3, ...),
        MockSession(date: "Jan 21", shotCount: 8, ...)
    ]
    
    static let sampleShots = [
        Shot(time: 0.5, type: .forehand, score: 6.2, issue: "Late contact"),
        Shot(time: 2.1, type: .backhand, score: 7.8, issue: "Good form"),
        Shot(time: 3.8, type: .forehand, score: 5.9, issue: "Poor follow-through")
    ]
}
```

### Testing Checklist
- [x] App launches to camera view
- [x] Can navigate to history
- [x] Can tap session to see analysis
- [x] Back buttons work correctly
- [ ] UI matches design specs

---

## Phase 2: Live Camera & Recording States
**Duration:** 6-8 hours  
**Goal:** Real camera feed with recording functionality

### Components to Build

#### 1. Camera Manager
```swift
class CameraManager: ObservableObject {
    @Published var isRecording = false
    @Published var recordingTime = 0.0
    @Published var permissionGranted = false
    
    private var captureSession: AVCaptureSession?
    private var videoOutput: AVCaptureMovieFileOutput?
    
    func startRecording() { }
    func stopRecording() { }
    func setupCamera() { }
}
```

#### 2. Recording States
```swift
enum RecordingState {
    case idle
    case recording(elapsed: TimeInterval)
    case processing
    case complete(videoURL: URL)
}
```

#### 3. Vision Skeleton Overlay
```swift
// Phase 2: Static skeleton image overlay
// Phase 5: Real Vision framework integration
struct SkeletonOverlay: View {
    var body: some View {
        Image(systemName: "figure.walk")
            .resizable()
            .opacity(0.3)
    }
}
```

#### 4. Recording Controls (Start/End)
```swift
// Replaces Start/Pause/Resume with Start and End controls
// Center button toggles between START and END depending on session state
// Visual style: pill button with color change (green for START, red for END)
```

### Mock Behavior
- Record for max 10 seconds (auto-stop)
- Show processing for 2 seconds after recording
- Navigate to analysis with recorded video
- Mock detection returns 3 predetermined swings

### Testing Checklist
- [x] Camera permission request works
- [x] Start/End controls update label and color
- [x] Timer shows during recording
- [x] Auto-stops at 10 seconds
- [x] Processing state appears
- [x] Navigates to analysis on END
- [x] Upload/History buttons hidden during active session

---

## Phase 3: Dynamic Analysis View
**Duration:** 8-10 hours  
**Goal:** Full video playback with synchronized insights, wired to the implemented timeline and shot chips.

### What’s already implemented (in code)
- **TimelineStrip**: 56pt glass band with shot markers mapped to time
  - Snap-to-nearest on tap/drag (≥32pt hit area)
  - VoiceOver: Adjustable with Previous/Next; descriptive labels
  - Haptics on selection
- **ShotChipsRow**: 44pt-high, horizontally scrollable chips + Prev/Next buttons
  - Chips show FH/BH short label and score; auto-centers selected chip
  - Haptics on selection
- **AnalysisView**: Integrates `VideoPlayerView(url:)`, `TimelineStrip`, `ShotChipsRow`, and an inline insight card (`insightCard`) that reflects the selected shot.

### Components to Build / Enhance

#### 1. Video Player Component (to be enhanced and bound)
```swift
struct VideoPlayerView: UIViewControllerRepresentable {
    let url: URL
    @Binding var currentTime: Double   // bind AVPlayer time -> UI
    @Binding var isPlaying: Bool       // play/pause control

    // Implement AVPlayer with periodic time observer
    // Support seeking to a time (from selection)
    // Basic play/pause controls, optional scrubbing
}
```

Integration in `AnalysisView` (target):
```swift
VideoPlayerView(url: url, currentTime: $currentTime, isPlaying: $isPlaying)
```

#### 2. Timeline + Chips (implemented; finalize sync behavior)
```swift
// TimelineStrip(duration: shots: selectedShotID: currentTime:)
// - Select shot → updates selectedShotID and currentTime
// - Adjustable for VoiceOver; announces position and score

// ShotChipsRow(shots: selectedShotID: onPrev: onNext:)
// - Prev/Next navigates shots; updates selectedShotID
// - Scrolls selected chip into view
```

Sync rules to implement:
- When `selectedShotID` changes (via timeline or chip), **seek** video to the shot’s `time`.
- While playing, as `currentTime` advances, **auto-select** the nearest shot at/after the current time (with hysteresis to avoid flicker).
- Drag on timeline → snap to nearest shot and seek the video.

#### 3. Insight Card (implemented; wire to selection)
- Inline in `AnalysisView` as `insightCard`.
- Shows selected shot’s type, score, and issue.
- Update content reactively on `selectedShotID` changes.

#### 4. Mock Swing Detector (no change)
```swift
class MockSwingDetector {
    static func detectSwings(in video: URL) -> [Shot] {
        // Return predetermined shots based on video length
        let duration = getVideoDuration(video)
        return [
            Shot(time: duration * 0.2, type: .forehand, /* ... */),
            Shot(time: duration * 0.5, type: .backhand, /* ... */),
            Shot(time: duration * 0.8, type: .forehand, /* ... */)
        ]
    }
}
```

### Visual Overlays (scaffold now, enhance later)
```swift
struct VideoOverlay: View {
    let currentShot: Shot?

    var body: some View {
        if let shot = currentShot {
            // Red dot at contact point
            // Green dot for ideal position
            // Arrow showing correction
        }
    }
}
```

### Testing Checklist
- [x] Video plays smoothly (AVPlayer-based `VideoPlayerView`)
- [x] Selecting a chip/timeline marker seeks video to shot time
- [x] While playing, selection auto-advances to the current shot
- [x] Timeline markers at correct positions (implemented)
- [x] Chips stay centered on selection (implemented)
- [ ] Visual overlays appear at the right time
- [ ] Auto-pause on critical issues (if applicable)

---

## Phase 4: Upload & Persistence
**Duration:** 4-6 hours  
**Goal:** Complete user flows with data persistence

### Components to Build

#### 1. Video Upload Handler
```swift
struct VideoUploader: View {
    @State private var showingPicker = false
    @State private var processingProgress = 0.0
    
    func processUploadedVideo(_ url: URL) {
        // Show progress for 3 seconds
        // Run mock detection
        // Save to history
        // Navigate to analysis
    }
}
```

Info.plist requirements for Photos access:

- Add `NSPhotoLibraryUsageDescription` with a clear message like:
  "Allow access to select a swing video from your library."
- We request read-only access via `PHPhotoLibrary.requestAuthorization(for: .readOnly)`.

#### 2. Session Storage
```swift
class SessionStore: ObservableObject {
    @Published var sessions: [MockSession] = []
    
    init() {
        loadFromUserDefaults()
    }
    
    func save(_ session: MockSession) {
        sessions.insert(session, at: 0)
        saveToUserDefaults()
    }
    
    func delete(_ session: MockSession) { }
}
```

#### 3. Thumbnail Generator
```swift
extension URL {
    func generateThumbnail() -> UIImage? {
        let asset = AVAsset(url: self)
        let generator = AVAssetImageGenerator(asset: asset)
        // Get frame at 1 second
        return thumbnail
    }
}
```

### Data Flow
```
Record/Upload → Process → Generate Thumbnail → Save Session → Show in History
```

### Testing Checklist
- [x] Can upload video from photo library
- [x] Progress bar during processing
- [x] New sessions appear in history
- [x] Thumbnails generate correctly (1s frame)
- [x] Sessions persist across app launches
- [ ] Can delete sessions

---

## Phase 5: Polish & Production Ready
**Duration:** 4-6 hours  
**Goal:** Complete UX with all edge cases handled

### Components to Polish

#### 1. Loading States
```swift
struct LoadingView: View {
    let message: String
    var body: some View {
        // < 1s: Nothing
        // 1-3s: Spinner
        // > 3s: Message
    }
}
```

#### 2. Error Handling
```swift
enum AppError {
    case noSwingsDetected
    case cameraPermissionDenied
    case uploadFailed
    case processingFailed
    
    var userMessage: String { }
    var recoveryAction: String { }
}
```

#### 3. Empty States
```swift
struct EmptyStateView: View {
    let type: EmptyStateType
    
    var body: some View {
        // Icon + Message + Action button
        // "No sessions yet. Record your first swing!"
    }
}
```

#### 4. Animations
```swift
// Spring animations throughout
.animation(.spring(response: 0.3, dampingFraction: 0.8), value: state)

// Haptic feedback
UIImpactFeedbackGenerator(style: .medium).impactOccurred()

// Button press states
.scaleEffect(isPressed ? 0.95 : 1.0)
```

#### 5. Share Functionality
```swift
struct ShareSheet: UIViewControllerRepresentable {
    let video: URL
    let insights: [String]
    
    // Create shareable video with overlay
    // Include coaching tips as text
}
```

### Edge Cases to Handle
- No swings detected → Show helpful message
- Camera permission denied → Settings prompt
- Video too short/long → Guidance
- Network issues (if using Gemini) → Fallback to local
- Storage full → Warning

### Testing Checklist
- [ ] All loading states appear correctly
- [ ] Errors show helpful recovery options
- [ ] Empty states guide user action
- [ ] Animations feel smooth
- [ ] Share creates branded video
- [ ] App feels polished and complete

---

## File Structure

```
TennisApp/
├── App/
│   └── TennisApp.swift
├── Views/
│   ├── CameraView.swift
│   ├── AnalysisView.swift
│   └── HistoryView.swift
├── Components/
│   ├── RecordButton.swift
│   ├── VideoPlayer.swift
│   ├── TimelineView.swift
│   ├── InsightCard.swift
│   └── VideoOverlay.swift
├── Models/
│   ├── Session.swift
│   ├── Shot.swift
│   └── SwingAnalysis.swift
├── Managers/
│   ├── CameraManager.swift
│   ├── VideoProcessor.swift
│   └── SessionStore.swift
├── Mock/
│   ├── MockDataProvider.swift
│   ├── MockSwingDetector.swift
│   └── MockAnalyzer.swift
└── Utils/
    ├── VideoUtils.swift
    └── Extensions.swift
```

---

## Success Metrics

### Phase 1 Complete When:
- All three screens exist and are navigable
- Mock data displays correctly

### Phase 2 Complete When:
- Can record real video
- States transition smoothly

### Phase 3 Complete When:
- Video analysis feels real
- Insights sync with playback

### Phase 4 Complete When:
- Full user journey works
- Data persists properly

### Phase 5 Complete When:
- No broken states
- Feels professional
- Ready for real users

---

## Migration to Real Implementation

When ready to add real swing detection:

1. **Replace MockSwingDetector** with real Vision framework implementation
2. **Swap MockAnalyzer** with Gemini API calls
3. **Keep all UI components unchanged**
4. **Add network layer for API communication**

The mock layer acts as a contract - real implementations must match the same interfaces.